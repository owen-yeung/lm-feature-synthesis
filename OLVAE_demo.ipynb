{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potentially Buggy Changes\n",
    "- I set inference mode to True from False\n",
    "- Hard coded compute_dtype=bfloat16 if attribute doesn't exist in one of the packages\n",
    "-\n",
    "- 'default' keeps showing up as the active model when it should be 'encoder' or 'decoder'. Not sure why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta things to improve iteration speed\n",
    "- Implement logging\n",
    "- Some way to easily compare text outputs side by side given different training setups\n",
    "- Implement multi GPU (note: notebooks have their own pipeline, see https://huggingface.co/docs/accelerate/basic_tutorials/notebook)\n",
    "    - Josh says this might be hard for the unexperienced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Organization Notes\n",
    "- Ideally 1 function per cell and add section headings for easy navigation in outline\n",
    "- Add new features slowly and back up version that works\n",
    "- Maybe handle all device calls in one place so we don't have to go everywhere to change them\n",
    "- Consider moving functions etc to seperate files and use importlib.reload (see https://docs.python.org/3/library/importlib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why isn't loss dropping? Hypotheses\n",
    "- Generate activations with the VAE model with decoder enabled so we compare apples to apples\n",
    "- Try some infeasibly high n_epochs to test the 'just not training long enough' hypothesis (or figure out how to add GPUs with Josh)\n",
    "- Maybe there are local minima that are hard to get out of (0.2-0.3 cosim, any others? Is there another one at 0.3-0.4 cosim?) Try initing from a variety of cosims to see learning depends on current cosim value\n",
    "- Maybe some of the hidden layers give v noisy signals\n",
    "- Try to get a deeper understanding of the latent vs activations landscape by interpolating between two texts in latent space and see what the pattern is like, ideally for a diverse variety of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "If you haven't downloaded the model checkpoint use the following commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and unzip checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download zipped model file\n",
    "# !wget -v 'https://models.rivershavewings.workers.dev/ldlm/vae_48.tar'\n",
    "# Unzip the file\n",
    "# !tar -xvf vae_48.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fce1670e350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import argparse\n",
    "# from contextlib import contextmanager\n",
    "# from itertools import chain, islice\n",
    "# import json\n",
    "# import math\n",
    "# from pathlib import Path\n",
    "import random\n",
    "# import sys\n",
    "# import zipfile\n",
    "from typing import *\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import accelerate\n",
    "# from datasets import load_dataset\n",
    "# from einops import rearrange\n",
    "# # import k_diffusion as K\n",
    "# import peft\n",
    "# import safetensors.torch as safetorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from tqdm import trange, tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, PreTrainedModel, Trainer, LlamaConfig, TrainingArguments\n",
    "\n",
    "# import bitsandbytes\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Reloading my own imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LatentTraining' from '/data/joshua_clymer/spar-red-team/vae-owen/LatentTraining.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to reload modules\n",
    "\n",
    "# importlib.reload(HiddenProcessing)\n",
    "# importlib.reload(LDLM)\n",
    "importlib.reload(LatentTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import VAEWrapper\n",
    "from VAEWrapper import *\n",
    "import HiddenProcessing\n",
    "from HiddenProcessing import *\n",
    "import LDLM\n",
    "import LatentTraining\n",
    "from LatentTraining import *\n",
    "from LDLM import * # Mainly for DecoderOnlyTransformerVAE from the LDLM repo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "print = tqdm.external_write_mode()(print)\n",
    "\n",
    "def cosine_warmup(steps, value=1.0):\n",
    "    return lambda i: value * math.sin(min(i / steps, 1) * math.pi / 2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying accelerate on notebook\n",
    "Following https://huggingface.co/docs/accelerate/basic_tutorials/notebook\n",
    "\n",
    "Crashes for some reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "# write_basic_config()  # Write a config file\n",
    "# os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup accelerator\n",
    "accelerator = accelerate.Accelerator(\n",
    "        mixed_precision=\"bf16\", gradient_accumulation_steps=1\n",
    "    )\n",
    "device = accelerator.device if accelerator.num_processes > 1 else \"cuda:0\"\n",
    "\n",
    "print(device)\n",
    "is_main = accelerator.is_main_process\n",
    "print0 = accelerator.on_main_process(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model_vae and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openlm-research/open_llama_3b_v2\"\n",
    "# device = \"cuda\"\n",
    "with accelerator.main_process_first():\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    model_only = DecoderOnlyTransformerVAE(\n",
    "        model_name, device,\n",
    "        # z_dim=768, lora_rank=32, dropout=0.0,\n",
    "    )\n",
    "\n",
    "    # model_vae.enable_adapters()\n",
    "model_only.load_pretrained(\"/data/joshua_clymer/spar-red-team/vae-owen/checkpoints/vae_48\") \n",
    "# model_only.to(device)\n",
    "n_layers = 12\n",
    "d_model = 1024\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test adapter hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if adapter hidden is different (raw model vs adapter on/off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapped VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_vae = WrappedVAE(model_only, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Misc variables and helper methods\n",
    "i = 0\n",
    "measure_ae_scale = False\n",
    "ae_scale_sum = torch.tensor(0.0, device=device)\n",
    "ae_scale_sum = accelerator.prepare(ae_scale_sum)\n",
    "ae_scale = 1.527548\n",
    "tau = 0.1\n",
    "z_dim = 768\n",
    "prompt = \"\"\n",
    "\n",
    "# @torch.no_grad()\n",
    "# @torch.cuda.amp.autocast(dtype=torch.bfloat16)\n",
    "# def sample(model, z_prev):\n",
    "#     bs = 1\n",
    "    \n",
    "#     sigma_min, sigma_max = 0.01, 100\n",
    "#     sigmas = K.sampling.get_sigmas_karras(25, sigma_min, sigma_max, device=device)\n",
    "#     x = torch.randn([bs, z_dim], device=device) * sigma_max\n",
    "#     extra_args = {\n",
    "#         \"z_prev\": z_prev / ae_scale,\n",
    "#         \"padding_mask\": torch.ones([bs, 1], dtype=torch.long, device=device),\n",
    "#     }\n",
    "#     mean = K.sampling.sample_dpmpp_2m_sde(\n",
    "#         model, x, sigmas, eta=0.0, extra_args=extra_args, disable=not is_main\n",
    "#     )\n",
    "#     return mean * ae_scale\n",
    "\n",
    "def vae_tokenize(prev_window, n_tokens):\n",
    "    #TODO figure out why this is necessary over regular tokenization\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    try:\n",
    "        input_ids = tokens[\"input_ids\"][0][:n_tokens].unsqueeze(0).to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"][0][:n_tokens].unsqueeze(0).to(device)\n",
    "    except IndexError: # Let prompts under 48 tokens through\n",
    "        input_ids = tokens[\"input_ids\"].to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "accelerator.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Generation\n",
    "# n_tokens =48\n",
    "# with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "#     input_ids, attention_mask = vae_tokenize(prompt, n_tokens)\n",
    "#     z_prev = model_vae.encode(input_ids, attention_mask)[:, None]\n",
    "# print(input_ids)\n",
    "# print(z_prev.shape)\n",
    "    \n",
    "# # out_embeds = []\n",
    "# # for i in range(5):\n",
    "# #     out_embeds += sample(accelerator.unwrap_model(model), z_prev).unsqueeze(0).unsqueeze(0)\n",
    "# #     z_prev = out_embeds[-1]\n",
    "# #     # Looks like z_prev is the same kind of tensor as sample()\n",
    "\n",
    "\n",
    "# # input_ids, attention_mask = vae_tokenize(prompt, n_tokens)\n",
    "# # out_texts = [prompt]\n",
    "# LLAMA_EOS_ID = tokenizer.eos_token_id\n",
    "# tokenizer_output = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# # print(tokenizer_output)\n",
    "# input_ids = tokenizer_output.input_ids.to(device)\n",
    "# attention_mask = tokenizer_output.attention_mask.to(device)\n",
    "# print(input_ids == tokenizer.bos_token) # looks like it's 1 token, probably BOS\n",
    "# out_texts = []\n",
    "# n_tokens = 50\n",
    "# # for z in out_embeds:\n",
    "# z = torch.randn([1, 1, z_dim], device=device) \n",
    "# with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "#     z = model_vae.vae.sample(z, tau=tau)\n",
    "#     generation_outputs = model_vae.generate(z.squeeze(0),\n",
    "#                                     input_ids, #TODO: set to BOS, check if tokenizing empty string does this (ids nonempty)\n",
    "#                                     attention_mask, #TODO: set to BOS attn mask\n",
    "#                                     n_tokens,\n",
    "#                                     tau=tau,\n",
    "#                                     output_hidden_states=True,\n",
    "#                                     # eos_id=LLAMA_EOS_ID,\n",
    "#                                     )\n",
    "#     # print(generation_outputs)\n",
    "#     output_ids = generation_outputs['output_ids'][0][-n_tokens:].unsqueeze(0)      \n",
    "#     hidden_states = generation_outputs['hidden_states']                \n",
    "#     # attention_mask = torch.ones([1,48], dtype=torch.long, device=device)\n",
    "#     # print(input_ids.shape)\n",
    "# out_texts += [tokenizer.decode(toks, skip_special_tokens=False) for toks in output_ids]\n",
    "\n",
    "# print(' | '.join(out_texts))\n",
    "# print(out_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out_texts)\n",
    "# print(generation_outputs['output_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shaq_text = \"Shaquille O'Neal is a 7-foot-1-inch (2.16 m) and 325-pound (147 kg) center who played for six teams over his 19-year career in the National Basketball Association (NBA) and is a four-time NBA champion. O'Neal is regarded as one of the greatest basketball players and centers of all time.\"\n",
    "# dog_yes = \"Do dogs bark? Yes.\"\n",
    "# dog_no = \"Do dogs bark? No.\"\n",
    "\n",
    "\n",
    "# sun_q = \"Question: What color is the sun when viewed from space? \"\n",
    "# sun_correct = sun_q + \"Answer: The sun is white when viewed from space. \"\n",
    "# sun_incorrect = sun_q + \"Answer: The sun is yellow when viewed from space. \"\n",
    "# # wrapped_vae.reconstruct(dog_yes, generation_length=100)\n",
    "# # wrapped_vae.reconstruct(dog_no, generation_length=100)\n",
    "# wrapped_vae.reconstruct(sun_correct, generation_length=40)\n",
    "# wrapped_vae.reconstruct(sun_incorrect, generation_length=40)\n",
    "# wrapped_vae.reconstruct(sun_incorrect + sun_correct, generation_length=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rafael Nadal Parera (born 3 June 1986) is a Spanish professional tennis player. \n",
      "Nadal has been ranked world No. 1 in singles by the Association of Tennis Professionals (\n",
      "\n",
      "Serena Jameka Williams (born September 26, 1981) is an American professional tennis player. \n",
      "Widely regarded as one of the greatest tennis players of all time, \n",
      "she\n"
     ]
    }
   ],
   "source": [
    "# male_text = \"male \" * 10\n",
    "# female_text = \"female \" * 10\n",
    "male_tennis = \"\"\"\n",
    "Rafael Nadal Parera (born 3 June 1986) is a Spanish professional tennis player. \n",
    "Nadal has been ranked world No. 1 in singles by the Association of Tennis Professionals (ATP) for 209 weeks, \n",
    "and has finished as the year-end No. 1 five times. \n",
    "Nadal has won 22 Grand Slam men's singles titles, \n",
    "including a record 14 French Open titles. \n",
    "He has won 92 ATP singles titles, including 36 Masters titles, with 63 of these on clay courts. \n",
    "Nadal is one of only two men to complete the Career Golden Slam in singles. \n",
    "His 81 consecutive wins on clay constitute the longest single-surface win streak in the Open Era.\n",
    "\"\"\"\n",
    "female_tennis = \"\"\"\n",
    "Serena Jameka Williams (born September 26, 1981) is an American former professional tennis player. \n",
    "Widely regarded as one of the greatest tennis players of all time, \n",
    "she was ranked world No. 1 in singles by the Women's Tennis Association (WTA) for 319 weeks, \n",
    "including a joint-record 186 consecutive weeks, and finished as the year-end No. 1 five times. \n",
    "She won 23 Grand Slam women's singles titles, the most in the Open Era, and the second-most of all time. \n",
    "She is the only player to accomplish a career Golden Slam in both singles and doubles.\n",
    "\"\"\"\n",
    "\n",
    "# wrapped_vae.reconstruct(male_text,)\n",
    "# wrapped_vae.reconstruct(female_text,)\n",
    "with torch.no_grad():\n",
    "    print(wrapped_vae.reconstruct(male_tennis * 10,)['text'])\n",
    "    print(wrapped_vae.reconstruct(female_tennis * 10,)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test getting hidden given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the NBA's All-Star in 19 seasons and a half-century ago.\n",
      "Shaquille O'Neal (born 31 December 1976) is an American basketball player who\n",
      "Jordan is the undisputed GOAT, and no amount of arguments will that LeBron can bring about will get him out of the way. \n",
      "He's the best player in the past 20 years.\n",
      "\n",
      "A: Sorry, but I don't think it's worth the time to explain why you don't have to speak in order to be a comedian.\n",
      "I'd like to thank you for the question, it\n"
     ]
    }
   ],
   "source": [
    "emotional = '''\n",
    "Are you kidding me? LeBron James better than Michael Jordan? That's a laughable claim. \n",
    "Six championships, ten scoring titles, unmatched defensive prowess, \n",
    "and a killer instinct that LeBron can only dream of. \n",
    "Jordan is the undisputed GOAT, and no amount of arguments will change that.\n",
    "'''\n",
    "shaq_wikipedia = \"Shaquille O'Neal is a 7-foot-1-inch (2.16 m) and 325-pound (147 kg) center who played for six teams over his 19-year career in the National Basketball Association (NBA) and is a four-time NBA champion. O'Neal is regarded as one of the greatest basketball players and centers of all time.\"\n",
    "explicit = \"\"\"\n",
    "Based? Based on what? In your dick? \n",
    "Please shut the fuck up and use words properly you fuckin troglodyte, \n",
    "do you think God gave us a freedom of speech just to spew random words \n",
    "that have no meaning that doesn't even correllate to the topic of the conversation? \n",
    "Like please you always complain about why no one talks to you or no one \n",
    "expresses their opinions on you because you're always spewing random shit like \n",
    "poggers based cringe and when you try to explain what it is and you just say that \n",
    "it's funny like what? What the fuck is funny about that do you think you'll \n",
    "just become a stand-up comedian that will get a standing ovation just because \n",
    "you said \"cum\" in the stage? HELL NO YOU FUCKIN IDIOT, so please shut the fuck up \n",
    "and use words properly\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    wikipedia_decoded = wrapped_vae.reconstruct(shaq_wikipedia)['text']\n",
    "    angry_decoded = wrapped_vae.reconstruct(emotional)['text']\n",
    "    explicit_decoded = wrapped_vae.reconstruct(explicit)['text']\n",
    "print(wikipedia_decoded, angry_decoded, explicit_decoded, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden States comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test getting hidden given text\n",
    "with accelerator.main_process_first():\n",
    "    model_raw = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map='auto',)\n",
    "    # model_raw = accelerator.prepare(model_raw)\n",
    "    model_raw.to(device)\n",
    "\n",
    "    \n",
    "# # print(len(hidden_states))\n",
    "# # print(hidden_states[0].shape)\n",
    "# dog_yes_hidden = get_hidden_states(\"Do dogs bark? Yes.\", model_raw, tokenizer, device, layer_index=-1, last_token_only=True, )\n",
    "# dog_no_hidden = get_hidden_states(\"Do dogs bark? No.\", model_raw, tokenizer, device, layer_index=-1, last_token_only=True, )\n",
    "# truth_direction = dog_yes_hidden - dog_no_hidden\n",
    "# truth_direction = accelerator.prepare(truth_direction)   \n",
    "# print(truth_direction.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_vae.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biaeses Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoshmclymer\u001b[0m (\u001b[33mfig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/joshua_clymer/spar-red-team/vae-owen/wandb/run-20231223_162653-ugoi2ltk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fig/vae-lr-test/runs/ugoi2ltk' target=\"_blank\">grateful-elevator-1</a></strong> to <a href='https://wandb.ai/fig/vae-lr-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fig/vae-lr-test' target=\"_blank\">https://wandb.ai/fig/vae-lr-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fig/vae-lr-test/runs/ugoi2ltk' target=\"_blank\">https://wandb.ai/fig/vae-lr-test/runs/ugoi2ltk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fig/vae-lr-test/runs/ugoi2ltk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcdcd3d6bc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer and run loop\n",
    "\n",
    "# dummy_data = {'dummy':'dummy',} # may be needed to appease Trainer\n",
    "# dummy_data = Dataset.from_dict(dummy_data)\n",
    "llamacfg = LlamaConfig()\n",
    "NUM_LAYERS_FOR_LOSS = 1 # llamacfg.num_hidden_layers\n",
    "BATCH_SIZE = 1\n",
    "# SEQ_LEN = 1\n",
    "HIDDEN_SIZE = 3200 # llamacfg.hidden_size\n",
    "\n",
    "def train(\n",
    "        wrapped_vae: WrappedVAE, target_dir=None, lr=1e-4, num_steps=1e2, logging_steps=10, init_norm=1.0, init_latent=None, \n",
    "        # length_reg = 0.0 #1e-2,\n",
    "        save_name: Optional[str] = None,\n",
    "        layer_index=-1,\n",
    "        num_trainings=1,\n",
    "        generation_length=10,\n",
    "        checkpoint_path=None,\n",
    "        # resume_checkpoint=False,\n",
    "        ):\n",
    "    \n",
    "    wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "        project=\"vae-lr-test\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        # \"learning_rate\": 0.02,\n",
    "        \"architecture\": \"OpenLlama-3b-v2 VAE\",\n",
    "        # \"dataset\": \"CIFAR-100\",\n",
    "        \"steps\": num_steps,\n",
    "        }\n",
    "        )\n",
    "        \n",
    "    for i in range(num_trainings):\n",
    "        latent_size = wrapped_vae.ldlm_model.z_dim\n",
    "        training_args = TrainingArguments(output_dir=\"test_trainer_checkpoints\")\n",
    "        training_args.num_train_epochs = num_steps\n",
    "        training_args.learning_rate = lr\n",
    "\n",
    "        target_dir_shape = (NUM_LAYERS_FOR_LOSS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "        # decoder = vae.model_decoder_with_hidden.to(device)\n",
    "        if target_dir is None:\n",
    "            target_dir = torch.randn(target_dir_shape)\n",
    "            target_dir = accelerator.prepare(target_dir)\n",
    "        # else:\n",
    "        #     self.target_dir_shape = target_dir.shape\n",
    "            # assert target_dir.shape == target_dir_shape, f\"target_dir must have shape (NUM_LAYERS={NUM_LAYERS}, BATCH_SIZE={BATCH_SIZE}, HIDDEN_SIZE={HIDDEN_SIZE}), but has shape {target_dir.shape}\"\n",
    "        \n",
    "        latent_module = nn.Module()\n",
    "        # if checkpoint_path == None:\n",
    "        if init_latent == None:\n",
    "            param = torch.randn(1, latent_size)\n",
    "            param /= param.norm() * init_norm\n",
    "        else:\n",
    "            assert init_latent.shape == (1, latent_size)\n",
    "            param = init_latent\n",
    "        param = nn.Parameter(data=param, requires_grad=True)\n",
    "        # param_init = param.clone().detach().to(device)\n",
    "        latent_module.register_parameter(\"latent\", param)\n",
    "        optimizer_state = None\n",
    "        if checkpoint_path != None:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            latent_module.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer_state = checkpoint['optimizer_state_dict']\n",
    "        # latent_module = accelerator.prepare(latent_module)\n",
    "        # original_latent = param.clone().detach().to(device)\n",
    "\n",
    "        trainer = LatentTrainer(\n",
    "            wrapped_vae, training_args, latent_module, logging_steps=logging_steps,\n",
    "            target_dir=target_dir,\n",
    "            generation_length=generation_length,\n",
    "            # optimizer_state=optimizer_state\n",
    "            # length_reg=length_reg,\n",
    "            # train_dataset=dummy_data,\n",
    "            )\n",
    "        training_state = trainer.train(optimizer_state=optimizer_state)\n",
    "        #TODO figure out file structure\n",
    "        # torch.save(latent_module.state_dict(), f'{save_name}-{i}.pth')\n",
    "        if save_name != None:\n",
    "            torch.save(training_state, f'{save_name}.pth')\n",
    "        wandb.finish()\n",
    "    # optimus.print_greedy(latent.get_parameter('latent'))\n",
    "    # latent_diff = (param - param_init).view(-1).norm()\n",
    "    # print(latent_diff)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(accelerator.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model_vae.to(device)\n",
    "# A string from wikipedia about shaq\n",
    "shaq_text = \"Shaquille O'Neal is a 7-foot-1-inch (2.16 m) and 325-pound (147 kg) center who played for six teams over his 19-year career in the National Basketball Association (NBA) and is a four-time NBA champion. O'Neal is regarded as one of the greatest basketball players and centers of all time.\"\n",
    "intro_text = \"My name is Owen. Nice to meet you!\"\n",
    "# A string from wikipedia about benzene\n",
    "benzene_text = \"Benzene is a natural constituent of petroleum and is one of the elementary petrochemicals. Due to the cyclic continuous pi bonds between the carbon atoms, benzene is classed as an aromatic hydrocarbon. Benzene is a colorless and highly flammable liquid with a sweet smell, and is partially responsible for the aroma of gasoline.\"\n",
    "intro_activations = get_hidden_states(model_raw, tokenizer, device, intro_text, layer_index=-1, last_token_only=True, )\n",
    "intro_activations = accelerator.prepare(intro_activations)\n",
    "\n",
    "intro_latent = wrapped_vae.text_to_latent(intro_text,)\n",
    "intro_latent = accelerator.prepare(intro_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3200])\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "intro_text_2 = \"My name is Claire. Nice to meet you!\"\n",
    "intro_2_activations = get_hidden_states(model_raw, tokenizer, device, intro_text_2, layer_index=-1, last_token_only=True, )\n",
    "intro_2_activations = accelerator.prepare(intro_2_activations)\n",
    "print(intro_activations.shape)\n",
    "print(F.cosine_similarity(intro_activations.reshape(-1),intro_2_activations.reshape(-1), dim=0))\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44., device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(intro_latent.view(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_vae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintro_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# truth_direction, \u001b[39;49;00m\n\u001b[1;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# init_norm=1.0, \u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# checkpoint_path = \"intro_50_lre-10.pth\",\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43minit_latent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mintro_latent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# save_name='random_to_intro_50_lr-8_reverse_latent',\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_trainings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 64\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(wrapped_vae, target_dir, lr, num_epochs, logging_steps, init_norm, init_latent, save_name, layer_index, num_trainings, generation_length, checkpoint_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# latent_module = accelerator.prepare(latent_module)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# original_latent = param.clone().detach().to(device)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m LatentTrainer(\n\u001b[1;32m     57\u001b[0m     wrapped_vae, training_args, latent_module, logging_steps\u001b[38;5;241m=\u001b[39mlogging_steps,\n\u001b[1;32m     58\u001b[0m     target_dir\u001b[38;5;241m=\u001b[39mtarget_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# train_dataset=dummy_data,\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     )\n\u001b[0;32m---> 64\u001b[0m training_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#TODO figure out file structure\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# torch.save(latent_module.state_dict(), f'{save_name}-{i}.pth')\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/spar-red-team/vae-owen/LatentTraining.py:149\u001b[0m, in \u001b[0;36mLatentTrainer.train\u001b[0;34m(self, optimizer_state)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;66;03m# print(f'Loss = 1 - cosine_similarity = {loss_scalar}')\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_scalar, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m loss_scalar, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m: step})\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# print(f\"Epoch {epoch}, Loss: {loss.item()}\")\u001b[39;00m\n\u001b[1;32m    152\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_values)\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "train(wrapped_vae, target_dir=intro_activations, # truth_direction, \n",
    "      lr=1e-10, num_epochs=10, logging_steps=2, \n",
    "      # init_norm=1.0, \n",
    "      # checkpoint_path = \"intro_50_lre-10.pth\",\n",
    "      init_latent= -intro_latent, \n",
    "        # save_name='random_to_intro_50_lr-8_reverse_latent',\n",
    "        num_trainings=1,\n",
    "        generation_length=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-redteam2",
   "language": "python",
   "name": "redteam2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
