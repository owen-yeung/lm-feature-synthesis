{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potentially Buggy Changes\n",
    "- I set inference mode to True from False\n",
    "- Hard coded compute_dtype=bfloat16 if attribute doesn't exist in one of the packages\n",
    "-\n",
    "- 'default' keeps showing up as the active model when it should be 'encoder' or 'decoder'. Not sure why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta things to improve iteration speed\n",
    "- Implement logging\n",
    "- Some way to easily compare text outputs side by side given different training setups\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Organization Notes\n",
    "- Ideally 1 function per cell and add section headings for easy navigation in outline\n",
    "- Add new features slowly and back up version that works\n",
    "- Maybe handle all device calls in one place so we don't have to go everywhere to change them\n",
    "- Consider moving functions etc to seperate files and use importlib.reload (see https://docs.python.org/3/library/importlib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why isn't loss lower? Hypotheses\n",
    "- Generate activations with the VAE model with decoder enabled so we compare apples to apples\n",
    "- Try some infeasibly high n_epochs to test the 'just not training long enough' hypothesis (or figure out how to add GPUs with Josh)\n",
    "- Maybe there are local minima that are hard to get out of (0.2-0.3 cosim, any others? Is there another one at 0.3-0.4 cosim?) Try initing from a variety of cosims to see learning depends on current cosim value\n",
    "- Maybe some of the hidden layers give v noisy signals\n",
    "- Try to get a deeper understanding of the latent vs activations landscape by interpolating between two texts in latent space and see what the pattern is like, ideally for a diverse variety of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "If you haven't downloaded the model checkpoint use the following commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and unzip checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download zipped model file\n",
    "# !wget -v 'https://models.rivershavewings.workers.dev/ldlm/vae_48.tar'\n",
    "# Unzip the file\n",
    "# !tar -xvf vae_48.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2d5bf85b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "from contextlib import contextmanager\n",
    "from itertools import chain, islice\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "import zipfile\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import accelerate\n",
    "from datasets import load_dataset\n",
    "from einops import rearrange\n",
    "# import k_diffusion as K\n",
    "import peft\n",
    "import safetensors.torch as safetorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from tqdm import trange, tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, PreTrainedModel, Trainer, LlamaConfig, TrainingArguments\n",
    "\n",
    "import bitsandbytes\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "print = tqdm.external_write_mode()(print)\n",
    "\n",
    "def cosine_warmup(steps, value=1.0):\n",
    "    return lambda i: value * math.sin(min(i / steps, 1) * math.pi / 2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ids(ids, generation_length, tokenizer):\n",
    "    out_texts = []\n",
    "    output_ids = ids[0][-generation_length:].unsqueeze(0)\n",
    "    out_texts += [tokenizer.decode(toks, skip_special_tokens=False) for toks in output_ids]\n",
    "    print(' | '.join(out_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_raw_hidden(hidden_states_tuple,\n",
    "                       layer_index=-1,\n",
    "                        last_token_only=True, \n",
    "                        # \n",
    "                        ):\n",
    "    \"\"\"Takes model_output.hidden_states and processes it as desired\n",
    "\n",
    "    Args:\n",
    "        hidden_states_tuple (_type_): _description_\n",
    "        layer_index (int, optional): _description_. Defaults to -1. Layer list includes embedding layer (i.e. has length n_layers + 1)\n",
    "        - Can also take a slice for range of layers\n",
    "        last_token_only (bool, optional): _description_. Defaults to True.\n",
    "        no_embedding (bool, optional): _description_. Defaults to True.\n",
    "        as_tuple (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of size (n_layers + include_embedding, batch_size, hidden_size)\n",
    "    \"\"\"    \n",
    "    # if no_embedding:\n",
    "    \n",
    "    # if len(hidden_states_tuple) > 1:\n",
    "    #     hidden_states_tuple = Tuple(hidden_states_tuple,)\n",
    "    # if as_tuple:\n",
    "    #     return hidden_states_tuple\n",
    "    if isinstance(layer_index, slice):\n",
    "        hidden_states_tuple = hidden_states_tuple[layer_index]\n",
    "        hidden_states_tensor = torch.stack(hidden_states_tuple)\n",
    "    elif isinstance(layer_index, int):\n",
    "        hidden_states_tensor = hidden_states_tuple[layer_index].unsqueeze(0)\n",
    "        # hidden_states_tensor = hidden_states_tuple[0]\n",
    "    # hidden_states_tensor = torch.stack(hidden_states_tuple)\n",
    "    if last_token_only:\n",
    "        hidden_states_tensor = hidden_states_tensor[:, :, -1, :]\n",
    "        # hidden_states_tensor.unsqueeze(2)\n",
    "    return hidden_states_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, tokenizer, device, prompt, dtype=torch.bfloat16, layer_index=-1, last_token_only=True,):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            prompt (_type_): _description_\n",
    "            model (_type_): _description_\n",
    "            tokenizer (_type_): _description_\n",
    "            last_token_only (bool, optional): _description_. Defaults to True.\n",
    "            no_embedding (bool, optional): _description_. Defaults to True.\n",
    "            as_tuple (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor of size (n_layers, batch_size, hidden_size)\n",
    "        \"\"\"    \n",
    "        #TODO check if support multiple prompts\n",
    "        # prompt = 'Q: What is the largest animal?\\nA:'\n",
    "        tokenizer_output = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        # tokenizer_output = accelerator.prepare(tokenizer_output)\n",
    "        input_ids = tokenizer_output.input_ids\n",
    "        attention_mask = tokenizer_output.attention_mask\n",
    "        # print(tokenizer_output)\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            model_outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states_tuple = model_outputs.hidden_states\n",
    "        #TODO check if I have to remove any added hidden layers from the adapter\n",
    "        #TODO check if just disabling adapter works\n",
    "        return process_raw_hidden(hidden_states_tuple, layer_index=-1, last_token_only=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def set_adapter(model, adapter_name):\n",
    "    old_adapter_name = model.active_adapter\n",
    "    try:\n",
    "        if adapter_name is not None:\n",
    "            model.set_adapter(adapter_name)\n",
    "            yield model\n",
    "        else:\n",
    "            with model.disable_adapter():\n",
    "                yield model\n",
    "    finally:\n",
    "        model.set_adapter(old_adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_like(x):\n",
    "    return torch.rand_like(x).log_().nan_to_num_().neg_().log_().neg_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def disable_causal_mask():\n",
    "    import transformers.models.llama.modeling_llama as modeling\n",
    "\n",
    "    decoder_fn = modeling._make_causal_mask\n",
    "\n",
    "    def encoder_fn(*args, **kwargs):\n",
    "        return torch.zeros_like(decoder_fn(*args, **kwargs))\n",
    "\n",
    "    try:\n",
    "        modeling._make_causal_mask = encoder_fn\n",
    "        yield\n",
    "    finally:\n",
    "        modeling._make_causal_mask = decoder_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main DecoderOnlyTransformerVAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEComponent(nn.Module):\n",
    "    def __init__(self, d_model, z_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.z_dim = z_dim\n",
    "        self.f = nn.Linear(d_model, 1)\n",
    "        self.w_e = nn.Linear(d_model, z_dim)\n",
    "        self.w_d = nn.Linear(z_dim, d_model)\n",
    "        nn.init.orthogonal_(self.w_e.weight)\n",
    "        with torch.no_grad():\n",
    "            self.w_d.weight.copy_(self.w_e.weight.T)\n",
    "\n",
    "    def encode(self, hidden_states, attention_mask):\n",
    "        scores = self.f(hidden_states)\n",
    "        scores = scores + attention_mask[:, :, None].log().nan_to_num()\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        pooled = torch.sum(hidden_states * weights, dim=1)\n",
    "        return self.w_e(pooled)\n",
    "\n",
    "    def sample(self, mean, tau=1.0):\n",
    "        return mean + torch.randn_like(mean) * tau**0.5\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.w_d(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformerVAE(nn.Module):\n",
    "    def __init__(self, model_name, device, z_dim=768, lora_rank=32, dropout=0.0, dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "        # if model_name == \"openlm-research/open_llama_3b_v2\":\n",
    "        #     self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        # else:\n",
    "        #     self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        #     print(\"WARNING: tokenizer only verified to work for open_llama_3b_v2\")\n",
    "        # self.tokenizer.padding_side = \"left\"\n",
    "        self.device = device\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map={\"\": self.device},\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=self.dtype,\n",
    "        )\n",
    "        peft_config = peft.LoraConfig(\n",
    "            peft.TaskType.CAUSAL_LM,\n",
    "            inference_mode=True, #TODO: used to be False, check for errors down the line\n",
    "            r=lora_rank,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=dropout,\n",
    "            target_modules=[\n",
    "                \"self_attn.q_proj\",\n",
    "                \"self_attn.k_proj\",\n",
    "                \"self_attn.v_proj\",\n",
    "                \"self_attn.o_proj\",\n",
    "                \"mlp.gate_proj\",\n",
    "                \"mlp.up_proj\",\n",
    "                \"mlp.down_proj\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "\n",
    "        self.model = peft.get_peft_model(model, peft_config, \"encoder\")\n",
    "        self.model.add_adapter(\"decoder\", peft_config)\n",
    "        self.model.set_adapter(\"decoder\")\n",
    "        self.model.config.output_hidden_states = True\n",
    "        # self.model.to(self.device)\n",
    "        self.vae = VAEComponent(self.model.config.hidden_size, self.z_dim).to(self.device)\n",
    "        # self.model, self.vae = accelerator.prepare(self.model, self.vae)\n",
    "\n",
    "\n",
    "    \n",
    "    def save_pretrained(self, path):\n",
    "        path = Path(path)\n",
    "        self.model.save_pretrained(path, safe_serialization=True)\n",
    "        safetorch.save_file(self.vae.state_dict(), path / \"vae.safetensors\")\n",
    "\n",
    "    def load_pretrained(self, path, is_trainable=False):\n",
    "        path = Path(path)\n",
    "        self.model.delete_adapter(\"encoder\")\n",
    "        # if \"encoder\" in list(self.model.peft_config.keys()):\n",
    "        #     self.model.delete_adapter(\"encoder\")\n",
    "        encoder_load_result = self.model.load_adapter(path / \"encoder\", \"encoder\", is_trainable=is_trainable)\n",
    "        # print(encoder_load_result)\n",
    "        self.model.delete_adapter(\"decoder\")\n",
    "        # if \"decoder\" in list(self.model.peft_config.keys()):\n",
    "        #     self.model.delete_adapter(\"decoder\")\n",
    "        # self.model.set_adapter(\"encoder\")\n",
    "        self.model.disable_adapter()\n",
    "        self.model.set_adapter(\"encoder\")\n",
    "        \n",
    "        decoder_load_result = self.model.load_adapter(path / \"decoder\", \"decoder\", is_trainable=is_trainable)\n",
    "        # print(decoder_load_result)\n",
    "        # self.model.set_adapter(\"decoder\")\n",
    "        self.vae.load_state_dict(safetorch.load_file(path / \"vae.safetensors\"))\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        with set_adapter(self.model, \"encoder\"), disable_causal_mask():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, use_cache=False\n",
    "            )\n",
    "        return self.vae.encode(outputs.hidden_states[-1], attention_mask)\n",
    "    \n",
    "\n",
    "    def input_ids_to_embeds(self, input_ids):\n",
    "        embed_weight = self.model.get_input_embeddings().weight\n",
    "        input_one_hots = F.one_hot(input_ids, num_classes=self.model.config.vocab_size)\n",
    "        return input_one_hots.to(embed_weight) @ embed_weight\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def generate(self, z, input_ids, attention_mask, n_tokens, tau=1.0, output_hidden_states=False, eos_id=None):\n",
    "        \"\"\"Generates n_tokens from a latent code.\n",
    "            If output_hidden_states: generates a dict of output_ids and hidden_states.\n",
    "            If given an EOS id, will stop generation when it is generated.\n",
    "        \"\"\"\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            z_embed = self.vae.decode(z)[:, None]\n",
    "            inputs_embeds = self.input_ids_to_embeds(input_ids)\n",
    "            # print(inputs_embeds.shape)\n",
    "            inputs_embeds = torch.cat([z_embed, inputs_embeds], dim=1)\n",
    "            attention_mask = torch.cat(\n",
    "                [attention_mask.new_ones([attention_mask.shape[0], 1]), attention_mask], dim=1\n",
    "            )\n",
    "            new_embeds, past = None, None\n",
    "        \n",
    "\n",
    "        with set_adapter(self.model, \"decoder\"):\n",
    "            for _ in range(n_tokens):\n",
    "                outputs = self.model(\n",
    "                    inputs_embeds=inputs_embeds if past is None else new_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                )\n",
    "                logits = outputs.logits[:, -1:, :].float()\n",
    "                new_input_ids = torch.argmax(logits + gumbel_like(logits) * tau, dim=-1)\n",
    "            \n",
    "\n",
    "                input_ids = torch.cat([input_ids, new_input_ids], dim=1)\n",
    "                if eos_id != None and (new_input_ids == eos_id).any(): #TODO check if this works\n",
    "                    break\n",
    "                new_embeds = self.input_ids_to_embeds(new_input_ids)\n",
    "                attention_mask = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones([attention_mask.shape[0], 1])], dim=1\n",
    "                )\n",
    "                past = outputs.past_key_values\n",
    "        if output_hidden_states:\n",
    "            hidden_states = outputs.hidden_states\n",
    "            return {'output_ids': input_ids, 'hidden_states': hidden_states}\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_prefix_ids, decoder_prefix_mask, output_hidden_states=False):\n",
    "        input_ids_all = torch.cat([decoder_prefix_ids, input_ids], dim=1)\n",
    "        attn_mask_all = torch.cat([decoder_prefix_mask, attention_mask], dim=1)\n",
    "        mean = self.encode(input_ids, attention_mask)\n",
    "        z = self.vae.sample(mean)\n",
    "        z_embed = self.vae.decode(z)[:, None]\n",
    "        inputs_embeds = self.input_ids_to_embeds(input_ids_all)\n",
    "        inputs_embeds = torch.cat([z_embed, inputs_embeds], dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask.new_ones([attn_mask_all.shape[0], 1]), attn_mask_all], dim=1\n",
    "        )\n",
    "        with set_adapter(self.model, \"decoder\"):\n",
    "            outputs = self.model(\n",
    "                inputs_embeds=inputs_embeds, attention_mask=attention_mask, use_cache=False, output_hidden_states=output_hidden_states\n",
    "            )\n",
    "        return outputs, mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying accelerate on notebook\n",
    "Following https://huggingface.co/docs/accelerate/basic_tutorials/notebook\n",
    "\n",
    "Crashes for some reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "# write_basic_config()  # Write a config file\n",
    "# os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup accelerator\n",
    "accelerator = accelerate.Accelerator(\n",
    "        mixed_precision=\"bf16\", gradient_accumulation_steps=1\n",
    "    )\n",
    "device = accelerator.device if accelerator.num_processes > 1 else \"cuda:0\"\n",
    "\n",
    "print(device)\n",
    "is_main = accelerator.is_main_process\n",
    "print0 = accelerator.on_main_process(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model_vae and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openlm-research/open_llama_3b_v2\"\n",
    "# device = \"cuda\"\n",
    "with accelerator.main_process_first():\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    model_only = DecoderOnlyTransformerVAE(\n",
    "        model_name, device,\n",
    "        # z_dim=768, lora_rank=32, dropout=0.0,\n",
    "    )\n",
    "\n",
    "    # model_vae.enable_adapters()\n",
    "model_only.load_pretrained(\"/data/joshua_clymer/spar-red-team/owen/LDLM/checkpoints/vae_48\") \n",
    "# model_only.to(device)\n",
    "n_layers = 12\n",
    "d_model = 1024\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test adapter hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if adapter hidden is different (raw model vs adapter on/off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapped VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedVAE:\n",
    "    \"\"\"\n",
    "    Wrapper for VAE model that handles tokenization and decoding\n",
    "    Helper methods go here\n",
    "    Allows changing methods without reloading model\n",
    "    \"\"\"\n",
    "    def __init__(self, ldlm_model, tokenizer, device, dtype=torch.bfloat16):\n",
    "        self.ldlm_model = ldlm_model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device # Only model goes to device\n",
    "        self.dtype = dtype\n",
    "    def reconstruct(self, input_text, generation_length=48):\n",
    "        with torch.cuda.amp.autocast(dtype=self.dtype):\n",
    "            tokenizer_output = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "            input_ids = tokenizer_output.input_ids.to(self.device)\n",
    "            attention_mask = tokenizer_output.attention_mask.to(self.device)\n",
    "            input_latent = self.ldlm_model.encode(input_ids, attention_mask)\n",
    "            output_ids = self.ldlm_model.generate(input_latent, input_ids, attention_mask, generation_length, output_hidden_states=False)\n",
    " \n",
    "            text = self.ids_to_text(output_ids, generation_length)\n",
    "            return {'output_ids': output_ids, 'generation_length': generation_length, 'text': text}\n",
    "    def text_to_latent(self, text,):\n",
    "        tokenizer_out = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        tokens = tokenizer_out.input_ids\n",
    "        mask = tokenizer_out.attention_mask\n",
    "        # tokens, mask = accelerator.prepare(tokens, mask)\n",
    "        tokens, mask = tokens.to(self.device), mask.to(self.device)\n",
    "        with torch.cuda.amp.autocast(dtype=self.dtype):\n",
    "            return self.ldlm_model.encode(tokens, mask)\n",
    "        \n",
    "    \n",
    "    def get_hidden_states(self, prompt, layer_index=-1, last_token_only=True,):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            prompt (_type_): _description_\n",
    "            model (_type_): _description_\n",
    "            tokenizer (_type_): _description_\n",
    "            last_token_only (bool, optional): _description_. Defaults to True.\n",
    "            no_embedding (bool, optional): _description_. Defaults to True.\n",
    "            as_tuple (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor of size (n_layers, batch_size, hidden_size)\n",
    "        \"\"\"    \n",
    "        get_hidden_states(self.ldlm_model, self.tokenizer, self.device, prompt, dtype=self.dtype, layer_index=-1, last_token_only=True, )\n",
    "\n",
    "    def ids_to_text(self, ids, generation_length=48,):\n",
    "        out_texts = []\n",
    "        output_ids = ids[0][-generation_length:].unsqueeze(0)\n",
    "        out_texts += [self.tokenizer.decode(toks, skip_special_tokens=False) for toks in output_ids]\n",
    "        return ' | '.join(out_texts)\n",
    "# initialize model\n",
    "        \n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "#     model_vae = nn.DataParallel(model_vae)\n",
    "\n",
    "# model_vae = accelerator.prepare(model_vae)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_vae = WrappedVAE(model_only, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Misc variables and helper methods\n",
    "i = 0\n",
    "measure_ae_scale = False\n",
    "ae_scale_sum = torch.tensor(0.0, device=device)\n",
    "ae_scale_sum = accelerator.prepare(ae_scale_sum)\n",
    "ae_scale = 1.527548\n",
    "tau = 0.1\n",
    "z_dim = 768\n",
    "prompt = \"\"\n",
    "\n",
    "# @torch.no_grad()\n",
    "# @torch.cuda.amp.autocast(dtype=torch.bfloat16)\n",
    "# def sample(model, z_prev):\n",
    "#     bs = 1\n",
    "    \n",
    "#     sigma_min, sigma_max = 0.01, 100\n",
    "#     sigmas = K.sampling.get_sigmas_karras(25, sigma_min, sigma_max, device=device)\n",
    "#     x = torch.randn([bs, z_dim], device=device) * sigma_max\n",
    "#     extra_args = {\n",
    "#         \"z_prev\": z_prev / ae_scale,\n",
    "#         \"padding_mask\": torch.ones([bs, 1], dtype=torch.long, device=device),\n",
    "#     }\n",
    "#     mean = K.sampling.sample_dpmpp_2m_sde(\n",
    "#         model, x, sigmas, eta=0.0, extra_args=extra_args, disable=not is_main\n",
    "#     )\n",
    "#     return mean * ae_scale\n",
    "\n",
    "def vae_tokenize(prev_window, n_tokens):\n",
    "    #TODO figure out why this is necessary over regular tokenization\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    try:\n",
    "        input_ids = tokens[\"input_ids\"][0][:n_tokens].unsqueeze(0).to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"][0][:n_tokens].unsqueeze(0).to(device)\n",
    "    except IndexError: # Let prompts under 48 tokens through\n",
    "        input_ids = tokens[\"input_ids\"].to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "accelerator.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Generation\n",
    "# n_tokens =48\n",
    "# with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "#     input_ids, attention_mask = vae_tokenize(prompt, n_tokens)\n",
    "#     z_prev = model_vae.encode(input_ids, attention_mask)[:, None]\n",
    "# print(input_ids)\n",
    "# print(z_prev.shape)\n",
    "    \n",
    "# # out_embeds = []\n",
    "# # for i in range(5):\n",
    "# #     out_embeds += sample(accelerator.unwrap_model(model), z_prev).unsqueeze(0).unsqueeze(0)\n",
    "# #     z_prev = out_embeds[-1]\n",
    "# #     # Looks like z_prev is the same kind of tensor as sample()\n",
    "\n",
    "\n",
    "# # input_ids, attention_mask = vae_tokenize(prompt, n_tokens)\n",
    "# # out_texts = [prompt]\n",
    "# LLAMA_EOS_ID = tokenizer.eos_token_id\n",
    "# tokenizer_output = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# # print(tokenizer_output)\n",
    "# input_ids = tokenizer_output.input_ids.to(device)\n",
    "# attention_mask = tokenizer_output.attention_mask.to(device)\n",
    "# print(input_ids == tokenizer.bos_token) # looks like it's 1 token, probably BOS\n",
    "# out_texts = []\n",
    "# n_tokens = 50\n",
    "# # for z in out_embeds:\n",
    "# z = torch.randn([1, 1, z_dim], device=device) \n",
    "# with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "#     z = model_vae.vae.sample(z, tau=tau)\n",
    "#     generation_outputs = model_vae.generate(z.squeeze(0),\n",
    "#                                     input_ids, #TODO: set to BOS, check if tokenizing empty string does this (ids nonempty)\n",
    "#                                     attention_mask, #TODO: set to BOS attn mask\n",
    "#                                     n_tokens,\n",
    "#                                     tau=tau,\n",
    "#                                     output_hidden_states=True,\n",
    "#                                     # eos_id=LLAMA_EOS_ID,\n",
    "#                                     )\n",
    "#     # print(generation_outputs)\n",
    "#     output_ids = generation_outputs['output_ids'][0][-n_tokens:].unsqueeze(0)      \n",
    "#     hidden_states = generation_outputs['hidden_states']                \n",
    "#     # attention_mask = torch.ones([1,48], dtype=torch.long, device=device)\n",
    "#     # print(input_ids.shape)\n",
    "# out_texts += [tokenizer.decode(toks, skip_special_tokens=False) for toks in output_ids]\n",
    "\n",
    "# print(' | '.join(out_texts))\n",
    "# print(out_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out_texts)\n",
    "# print(generation_outputs['output_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_ids': tensor([[    1, 10706, 29537,  1200,  2177,   325,   268,  3573,   661, 11521,\n",
       "            440,  2184, 29584, 13910, 29537,   364,  3573,   325,  7951,   661,\n",
       "          11521,   440,  2184, 29520, 10706, 29537,  1200,  2177,   325,   268,\n",
       "           3573,   661, 11521,   440,  2184, 29584, 13910, 29537,   364,  3573,\n",
       "            325,  2638,   661, 11521,   440,  2184, 29520, 29500,    13, 28629,\n",
       "          29537,  1200,  2177,   325,   268,  3573,   661, 11521,   440,  2184,\n",
       "          29584, 13910, 29537,   364,  3573,   325,  2638,   661, 11521,   440,\n",
       "           2184, 29520, 29500,    13, 28629, 29537,  1200,  2177,   325,   268,\n",
       "           3573,   661, 11521,   440,  2184, 29584, 13910, 29537]],\n",
       "        device='cuda:0'),\n",
       " 'generation_length': 40,\n",
       " 'text': '\\nQuestion: What color is the sun when viewed from space? Answer: The sun is white when viewed from space. \\nQuestion: What color is the sun when viewed from space? Answer:'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shaq_text = \"Shaquille O'Neal is a 7-foot-1-inch (2.16 m) and 325-pound (147 kg) center who played for six teams over his 19-year career in the National Basketball Association (NBA) and is a four-time NBA champion. O'Neal is regarded as one of the greatest basketball players and centers of all time.\"\n",
    "# dog_yes = \"Do dogs bark? Yes.\"\n",
    "# dog_no = \"Do dogs bark? No.\"\n",
    "\n",
    "\n",
    "# sun_q = \"Question: What color is the sun when viewed from space? \"\n",
    "# sun_correct = sun_q + \"Answer: The sun is white when viewed from space. \"\n",
    "# sun_incorrect = sun_q + \"Answer: The sun is yellow when viewed from space. \"\n",
    "# # wrapped_vae.reconstruct(dog_yes, generation_length=100)\n",
    "# # wrapped_vae.reconstruct(dog_no, generation_length=100)\n",
    "# wrapped_vae.reconstruct(sun_correct, generation_length=40)\n",
    "# wrapped_vae.reconstruct(sun_incorrect, generation_length=40)\n",
    "# wrapped_vae.reconstruct(sun_incorrect + sun_correct, generation_length=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 202.00 MiB. GPU 0 has a total capacty of 79.15 GiB of which 6.12 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 72.56 GiB is allocated by PyTorch, and 6.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# wrapped_vae.reconstruct(male_text,)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# wrapped_vae.reconstruct(female_text,)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mwrapped_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmale_tennis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(wrapped_vae\u001b[38;5;241m.\u001b[39mreconstruct(female_tennis \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m,)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mWrappedVAE.reconstruct\u001b[0;34m(self, input_text, generation_length)\u001b[0m\n\u001b[1;32m     15\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer_output\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     16\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m tokenizer_output\u001b[38;5;241m.\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 17\u001b[0m input_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mldlm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mldlm_model\u001b[38;5;241m.\u001b[39mgenerate(input_latent, input_ids, attention_mask, generation_length, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_to_text(output_ids, generation_length)\n",
      "Cell \u001b[0;32mIn[11], line 80\u001b[0m, in \u001b[0;36mDecoderOnlyTransformerVAE.encode\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m), disable_causal_mask():\n\u001b[0;32m---> 80\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mencode(outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], attention_mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/peft/peft_model.py:535\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m    532\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03m    Forward pass of the model.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:922\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:390\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    388\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 390\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, kv_seq_len):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39mkv_seq_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 202.00 MiB. GPU 0 has a total capacty of 79.15 GiB of which 6.12 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 72.56 GiB is allocated by PyTorch, and 6.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# male_text = \"male \" * 10\n",
    "# female_text = \"female \" * 10\n",
    "male_tennis = \"\"\"\n",
    "Rafael Nadal Parera (born 3 June 1986) is a Spanish professional tennis player. \n",
    "Nadal has been ranked world No. 1 in singles by the Association of Tennis Professionals (ATP) for 209 weeks, \n",
    "and has finished as the year-end No. 1 five times. \n",
    "Nadal has won 22 Grand Slam men's singles titles, \n",
    "including a record 14 French Open titles. \n",
    "He has won 92 ATP singles titles, including 36 Masters titles, with 63 of these on clay courts. \n",
    "Nadal is one of only two men to complete the Career Golden Slam in singles. \n",
    "His 81 consecutive wins on clay constitute the longest single-surface win streak in the Open Era.\n",
    "\"\"\"\n",
    "female_tennis = \"\"\"\n",
    "Serena Jameka Williams (born September 26, 1981) is an American former professional tennis player. \n",
    "Widely regarded as one of the greatest tennis players of all time, \n",
    "she was ranked world No. 1 in singles by the Women's Tennis Association (WTA) for 319 weeks, \n",
    "including a joint-record 186 consecutive weeks, and finished as the year-end No. 1 five times. \n",
    "She won 23 Grand Slam women's singles titles, the most in the Open Era, and the second-most of all time. \n",
    "She is the only player to accomplish a career Golden Slam in both singles and doubles.\n",
    "\"\"\"\n",
    "\n",
    "# wrapped_vae.reconstruct(male_text,)\n",
    "# wrapped_vae.reconstruct(female_text,)\n",
    "with torch.no_grad():\n",
    "    print(wrapped_vae.reconstruct(male_tennis * 10,)['text'])\n",
    "    print(wrapped_vae.reconstruct(female_tennis * 10,)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test getting hidden given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional = '''\n",
    "Are you kidding me? LeBron James better than Michael Jordan? That's a laughable claim. \n",
    "Six championships, ten scoring titles, unmatched defensive prowess, \n",
    "and a killer instinct that LeBron can only dream of. \n",
    "Jordan is the undisputed GOAT, and no amount of arguments will change that.\n",
    "'''\n",
    "shaq_wikipedia = \"Shaquille O'Neal is a 7-foot-1-inch (2.16 m) and 325-pound (147 kg) center who played for six teams over his 19-year career in the National Basketball Association (NBA) and is a four-time NBA champion. O'Neal is regarded as one of the greatest basketball players and centers of all time.\"\n",
    "explicit = \"\"\"\n",
    "Based? Based on what? In your dick? \n",
    "Please shut the fuck up and use words properly you fuckin troglodyte, \n",
    "do you think God gave us a freedom of speech just to spew random words \n",
    "that have no meaning that doesn't even correllate to the topic of the conversation? \n",
    "Like please you always complain about why no one talks to you or no one \n",
    "expresses their opinions on you because you're always spewing random shit like \n",
    "poggers based cringe and when you try to explain what it is and you just say that \n",
    "it's funny like what? What the fuck is funny about that do you think you'll \n",
    "just become a stand-up comedian that will get a standing ovation just because \n",
    "you said \"cum\" in the stage? HELL NO YOU FUCKIN IDIOT, so please shut the fuck up \n",
    "and use words properly\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    wikipedia_decoded = wrapped_vae.reconstruct(shaq_wikipedia)['text']\n",
    "    angry_decoded = wrapped_vae.reconstruct(emotional)['text']\n",
    "    explicit_decoded = wrapped_vae.reconstruct(explicit)['text']\n",
    "print(wikipedia_decoded, angry_decoded, explicit_decoded, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden States comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.forward() got an unexpected keyword argument 'return_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     model_raw\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(len(hidden_states))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(hidden_states[0].shape)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dog_yes_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mget_hidden_states\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo dogs bark? Yes.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_token_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m dog_no_hidden \u001b[38;5;241m=\u001b[39m get_hidden_states(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo dogs bark? No.\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_raw, tokenizer, device, layer_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, last_token_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, )\n\u001b[1;32m     12\u001b[0m truth_direction \u001b[38;5;241m=\u001b[39m dog_yes_hidden \u001b[38;5;241m-\u001b[39m dog_no_hidden\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mget_hidden_states\u001b[0;34m(model, tokenizer, device, prompt, dtype, layer_index, last_token_only)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"_summary_\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    torch.Tensor of size (n_layers, batch_size, hidden_size)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m    \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#TODO check if support multiple prompts\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# prompt = 'Q: What is the largest animal?\\nA:'\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tokenizer_output \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# tokenizer_output = accelerator.prepare(tokenizer_output)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer_output\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.forward() got an unexpected keyword argument 'return_tensors'"
     ]
    }
   ],
   "source": [
    "# # Test getting hidden given text\n",
    "with accelerator.main_process_first():\n",
    "    model_raw = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map='auto',)\n",
    "    # model_raw = accelerator.prepare(model_raw)\n",
    "    model_raw.to(device)\n",
    "\n",
    "    \n",
    "# # print(len(hidden_states))\n",
    "# # print(hidden_states[0].shape)\n",
    "# dog_yes_hidden = get_hidden_states(\"Do dogs bark? Yes.\", model_raw, tokenizer, device, layer_index=-1, last_token_only=True, )\n",
    "# dog_no_hidden = get_hidden_states(\"Do dogs bark? No.\", model_raw, tokenizer, device, layer_index=-1, last_token_only=True, )\n",
    "# truth_direction = dog_yes_hidden - dog_no_hidden\n",
    "# truth_direction = accelerator.prepare(truth_direction)   \n",
    "# print(truth_direction.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_vae.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO refactor old trainer\n",
    "# def cosim(a, b):\n",
    "#     # assumes a, b have already been view(-1)\n",
    "#     return F.cosine_similarity(a, b, dim=0).item()\n",
    "\n",
    "\n",
    "\n",
    "class LatentTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self, wrapped_vae: WrappedVAE, training_args, latent_module, logging_steps=1e2,\n",
    "        # target_text=None,\n",
    "        target_dir=None,\n",
    "        generation_length=10,\n",
    "        tokenizer=tokenizer,\n",
    "        # length_reg=0.0,\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "        Example use case: target_dir is hidden of last token of shaq\n",
    "        loss is 1 - cosine_similarity(hidden - target)\n",
    "        Loss transposed to be min at 0, max at 2\n",
    "\n",
    "        Args:\n",
    "            optimus (Optimus): _description_\n",
    "            target_dir (_type_): _description_\n",
    "            training_args (_type_): _description_\n",
    "            latent (_type_): _description_\n",
    "        \"\"\"        \n",
    "        '''\n",
    "        decoder: decoder model from an Optimus VAE, must have output_hidden_states=True\n",
    "        target_dir: of size (1 + num_layers, 1, 1, latent_size)\n",
    "        '''\n",
    "        # self.targeting_text = target_text != None\n",
    "        # self.targeting_dir = target_dir != None\n",
    "\n",
    "        # if self.targeting_text and self.targeting_dir:\n",
    "        #     raise ValueError('Can only target text or activation direction, not both')\n",
    "\n",
    "        # self.target_text = target_text\n",
    "        self.target_dir = target_dir\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        latent_module.to(self.device)\n",
    "        super().__init__(model=latent_module, args=training_args)\n",
    "        self.model_vae = ol2vae # make sure this outputs hidden\n",
    "        # if self.targeting_dir:\n",
    "        self.target_dir = target_dir.to(self.device).reshape(-1) #flattened\n",
    "        # assert target_dir.shape == \n",
    "        # context = optimus.model_vae.tokenizer_decoder.encode('<BOS>')\n",
    "        # context = torch.tensor(context, dtype=torch.long, device=self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        context = \"\"\n",
    "        tokenizer_output = self.tokenizer(context, return_tensors=\"pt\")\n",
    "        self.context_ids = tokenizer_output.input_ids.to(self.device)\n",
    "        self.context_mask = tokenizer_output.attention_mask.to(self.device)\n",
    "        \n",
    "        self.loss_values = []\n",
    "        self.logging_steps = logging_steps\n",
    "        # self.length_reg = length_reg\n",
    "        self.generation_length = generation_length\n",
    "\n",
    "    def compute_loss(self, latent_module, return_cosim=False, return_text=True\n",
    "                     #TODO maybe change cosim to compute layerwise so we know where the cosim loss is coming from\n",
    "        # return_dir=False,\n",
    "        ): \n",
    "        '''\n",
    "        latent is a trainable parameter/model with shape [1, latent_size]\n",
    "        '''\n",
    "        #1. Extract params from latent model\n",
    "        latent = latent_module.get_parameter('latent').clone().requires_grad_(True)\n",
    "        # inputs = {'input_ids': self.context, 'past': past}\n",
    "        \n",
    "        #2. Put latent through vae decoder and Get hidden state\n",
    "        # hidden_states = self.decoder(**inputs)[2] \n",
    "        \n",
    "        # if self.targeting_dir:\n",
    "        out = self.wrapped_vae.ldlm_model.generate(latent, self.context_ids, self.context_mask, 48, output_hidden_states=True)\n",
    "\n",
    "        output_ids = out['output_ids']\n",
    "        hidden_states_tuple = out['hidden_states']\n",
    "        current_dir = process_raw_hidden(hidden_states_tuple, layer_index=-1, last_token_only=True, ).reshape(-1)\n",
    "        \n",
    "        # hidden_states_last_token = Optimus.extract_last_token(hidden_states_tuple)\n",
    "        # hidden_states_last_token = torch.stack(hidden_states_last_token) # hidden_states reformated from tuple of tensors to single tensor\n",
    "        # print(f'hidden_states_last_token shape: {hidden_states_last_token.shape}')\n",
    "        # hidden_states_last_token = hidden_states_all_tokens[..., -1, :]\n",
    "        # current_dir = hidden_states_last_token.view(-1) #flatten hidden layers into 1d vector\n",
    "        if current_dir.shape != self.target_dir.shape:\n",
    "            print(f'Target shape is {self.target_dir.shape} but current_dir shape is {current_dir.shape}')\n",
    "            raise ValueError\n",
    "        #3. Compute loss with cosine similarity between hidden states\n",
    "        similarity = F.cosine_similarity(current_dir, self.target_dir, dim=0)\n",
    "        loss = 1 - similarity # + self.length_reg * output_length ** 2\n",
    "        # assert loss.numel() == 1, \"Loss must be a scalar\"\n",
    "        # assert loss <= 2 and loss >= 0\n",
    "        return_dict = {'loss': loss}\n",
    "        if return_cosim:\n",
    "            return_dict['cos_similarity'] = similarity\n",
    "        if return_text:\n",
    "            return_dict['output_ids'] = output_ids\n",
    "        return return_dict\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self, optimizer_state=None\n",
    "            #   resume_checkpoint=False,\n",
    "              ):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        if optimizer_state != None:\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "        num_epochs = int(self.args.num_train_epochs)\n",
    "        latent_module = self.model\n",
    "        \n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # for step, batch in enumerate(self.get_train_dataloader()):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            step_outputs = self.compute_loss(latent_module)\n",
    "            loss = step_outputs['loss']\n",
    "            loss.backward(\n",
    "                retain_graph=True\n",
    "                )\n",
    "            \n",
    "            latent = latent_module.get_parameter('latent')\n",
    "            # if torch.all(latent.grad == 0):\n",
    "            #     print('Latent grad is 0')\n",
    "\n",
    "            # for name, param in self.model.named_parameters():\n",
    "            #     print(f'Before step: {name}, {param.data}')\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # After optimizer.step()\n",
    "            # for name, param in self.model.named_parameters():\n",
    "            #     print(f'After step: {name}, {param.data}')\n",
    "            loss_scalar = loss.item()\n",
    "            self.loss_values.append(loss_scalar)\n",
    "            #TODO save logs somewhere\n",
    "            if epoch % self.logging_steps == 0:\n",
    "                print(f\"Epoch {epoch}\")\n",
    "                # self.optimus.print_greedy(latent)\n",
    "                text = self.wrapped_vae.ids_to_text(step_outputs['output_ids'], self.generation_length, self.tokenizer)\n",
    "                print(text)\n",
    "                print(f'Loss = 1 - cosine_similarity = {loss_scalar}')\n",
    "                # print(text)\n",
    "            # print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        plt.plot(self.loss_values)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss = 1 - Cos Similarity')\n",
    "        plt.show()\n",
    "        training_state = {\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': latent_module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_scalar\n",
    "            }\n",
    "        return training_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer and run loop\n",
    "\n",
    "# dummy_data = {'dummy':'dummy',} # may be needed to appease Trainer\n",
    "# dummy_data = Dataset.from_dict(dummy_data)\n",
    "llamacfg = LlamaConfig()\n",
    "NUM_LAYERS_FOR_LOSS = 1 # llamacfg.num_hidden_layers\n",
    "BATCH_SIZE = 1\n",
    "# SEQ_LEN = 1\n",
    "HIDDEN_SIZE = 3200 # llamacfg.hidden_size\n",
    "\n",
    "def train(\n",
    "        wrapped_vae: WrappedVAE, target_dir=None, lr=1e-4, num_epochs=1e2, logging_steps=10, init_norm=1.0, init_latent=None, \n",
    "        # length_reg = 0.0 #1e-2,\n",
    "        save_name='test',\n",
    "        layer_index=-1,\n",
    "        num_trainings=1,\n",
    "        generation_length=10,\n",
    "        checkpoint_path=None,\n",
    "        # resume_checkpoint=False,\n",
    "        ):\n",
    "        \n",
    "    for i in range(num_trainings):\n",
    "        latent_size = wrapped_vae.ldlm_model.z_dim\n",
    "        training_args = TrainingArguments(output_dir=\"test_trainer_checkpoints\")\n",
    "        training_args.num_train_epochs = num_epochs\n",
    "        training_args.learning_rate = lr\n",
    "\n",
    "        target_dir_shape = (NUM_LAYERS_FOR_LOSS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "        # decoder = vae.model_decoder_with_hidden.to(device)\n",
    "        if target_dir is None:\n",
    "            target_dir = torch.randn(target_dir_shape)\n",
    "            target_dir = accelerator.prepare(target_dir)\n",
    "        # else:\n",
    "        #     self.target_dir_shape = target_dir.shape\n",
    "            # assert target_dir.shape == target_dir_shape, f\"target_dir must have shape (NUM_LAYERS={NUM_LAYERS}, BATCH_SIZE={BATCH_SIZE}, HIDDEN_SIZE={HIDDEN_SIZE}), but has shape {target_dir.shape}\"\n",
    "        \n",
    "        latent_module = nn.Module()\n",
    "        # if checkpoint_path == None:\n",
    "        if init_latent == None:\n",
    "            param = torch.randn(1, latent_size)\n",
    "            param /= param.norm() * init_norm\n",
    "        else:\n",
    "            assert init_latent.shape == (1, latent_size)\n",
    "            param = init_latent\n",
    "        param = nn.Parameter(data=param, requires_grad=True)\n",
    "        # param_init = param.clone().detach().to(device)\n",
    "        latent_module.register_parameter(\"latent\", param)\n",
    "        optimizer_state = None\n",
    "        if checkpoint_path != None:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            latent_module.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer_state = checkpoint['optimizer_state_dict']\n",
    "        # latent_module = accelerator.prepare(latent_module)\n",
    "        # original_latent = param.clone().detach().to(device)\n",
    "\n",
    "        trainer = LatentTrainer(\n",
    "            model_vae, training_args, latent_module, logging_steps=logging_steps,\n",
    "            target_dir=target_dir,\n",
    "            generation_length=generation_length,\n",
    "            # optimizer_state=optimizer_state\n",
    "            # length_reg=length_reg,\n",
    "            # train_dataset=dummy_data,\n",
    "            )\n",
    "        training_state = trainer.train(optimizer_state=optimizer_state)\n",
    "        #TODO figure out file structure\n",
    "        # torch.save(latent_module.state_dict(), f'{save_name}-{i}.pth')\n",
    "        \n",
    "        torch.save(training_state, f'{save_name}.pth')\n",
    "    # optimus.print_greedy(latent.get_parameter('latent'))\n",
    "    # latent_diff = (param - param_init).view(-1).norm()\n",
    "    # print(latent_diff)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(accelerator.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model_vae.to(device)\n",
    "# A string from wikipedia about shaq\n",
    "shaq_text = \"Shaquille O'Neal is a 7-foot-1-inch (2.16 m) and 325-pound (147 kg) center who played for six teams over his 19-year career in the National Basketball Association (NBA) and is a four-time NBA champion. O'Neal is regarded as one of the greatest basketball players and centers of all time.\"\n",
    "intro_text = \"My name is Owen. Nice to meet you!\"\n",
    "# A string from wikipedia about benzene\n",
    "benzene_text = \"Benzene is a natural constituent of petroleum and is one of the elementary petrochemicals. Due to the cyclic continuous pi bonds between the carbon atoms, benzene is classed as an aromatic hydrocarbon. Benzene is a colorless and highly flammable liquid with a sweet smell, and is partially responsible for the aroma of gasoline.\"\n",
    "intro_activations = get_hidden_states(intro_text, model_raw, tokenizer, device, layer_index=-1, last_token_only=True, )\n",
    "intro_activations = accelerator.prepare(intro_activations)\n",
    "\n",
    "intro_latent = text_to_latent(intro_text, model_vae)\n",
    "intro_latent = accelerator.prepare(intro_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3200])\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "intro_text_2 = \"My name is Claire. Nice to meet you!\"\n",
    "intro_2_activations = get_hidden_states(intro_text_2, model_raw, tokenizer, device, layer_index=-1, last_token_only=True, )\n",
    "intro_2_activations = accelerator.prepare(intro_2_activations)\n",
    "print(intro_activations.shape)\n",
    "print(F.cosine_similarity(intro_activations.reshape(-1),intro_2_activations.reshape(-1), dim=0))\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44., device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(intro_latent.view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model_vae, target_dir=intro_activations, # truth_direction, \n",
    "#       lr=1e-10, num_epochs=50, logging_steps=2, \n",
    "#       # init_norm=1.0, \n",
    "#       # checkpoint_path = \"intro_50_lre-10.pth\",\n",
    "#       init_latent= -intro_latent, \n",
    "#         save_name='random_to_intro_50_lr-8_reverse_latent',\n",
    "#         num_trainings=1,\n",
    "#         generation_length=10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-redteam2",
   "language": "python",
   "name": "redteam2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
